# -*- coding: utf-8 -*-
"""Submission 2_Cellphone Recommendations System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m8TCCyYIq80l2og375ha8-lgBhjTRdl0

# Data Understanding

Tahap ini adalah langkah awal dalam proyek machine learning di mana tujuannya adalah untuk mengenal dan memahami dataset yang akan digunakan, termasuk memuat data dan melihat karakteristik dasarnya.
"""

import kagglehub
path = kagglehub.dataset_download("meirnizri/cellphones-recommendations")

print("Path to dataset files:", path)

import pandas as pd

cellphones_data_df = pd.read_csv('/kaggle/input/cellphones-recommendations/cellphones data.csv')
cellphones_ratings_df = pd.read_csv('/kaggle/input/cellphones-recommendations/cellphones ratings.csv')
cellphones_users_df = pd.read_csv('/kaggle/input/cellphones-recommendations/cellphones users.csv')

print('Jumlah data cellphones: ', len(cellphones_data_df.cellphone_id.unique()))
print('Jumlah data rating tiap cellphones: ', len(cellphones_ratings_df.cellphone_id.unique()))
print('Jumlah data pengguna cellphones: ', len(cellphones_users_df.user_id.unique()))

"""# Univariate EDA

Sel ini menandai dimulainya analisis terhadap setiap variabel secara individual untuk memahami distribusi, tipe data, dan informasi penting lainnya dari masing-masing fitur dalam dataset.
"""

cellphones_data_df.head()

cellphones_data_df.info()

print('Banyak data: ', len(cellphones_data_df.cellphone_id.unique()))
print('Jenis brand yang ada pada data cellphones: ', cellphones_data_df.brand.unique())

print(cellphones_users_df.shape)

cellphones_users_df.head()

cellphones_ratings_df.head()

cellphones_ratings_df.describe()

print('Jumlah userID: ', len(cellphones_ratings_df.user_id.unique()))
print('Jumlah placeID: ', len(cellphones_ratings_df.cellphone_id.unique()))
print('Jumlah data cellphones_ratings_df: ', len(cellphones_ratings_df))

"""# Data Preprocessing

Kode di bawah sel ini akan fokus pada penggabungan beberapa sumber data (misalnya, ratings, users, dan cellphones) menjadi satu dataframe yang komprehensif untuk analisis lebih lanjut.
"""

import numpy as np

cellphones_all = np.concatenate((
    cellphones_data_df.cellphone_id.unique(),
))

cellphones_all = np.sort(np.unique(cellphones_all))

print('Jumlah seluruh data cellphones berdasarkan user_id: ', len(cellphones_all))

user_all = np.concatenate((
    cellphones_users_df.user_id.unique(),
))

user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user: ', len(user_all))

cellphones_info = pd.concat([cellphones_data_df])

cellphones = pd.merge(cellphones_ratings_df, cellphones_info , on='cellphone_id', how='left')
cellphones

cellphones.isnull().sum()

cellphones.groupby('cellphone_id').sum()

all_cellphones_rate = cellphones_ratings_df
all_cellphones_rate

all_cellphones_name = pd.merge(all_cellphones_rate, cellphones_data_df[['cellphone_id','model', 'price','brand']], on='cellphone_id', how='left')

all_cellphones_name

all_cellphones = pd.merge(all_cellphones_name, cellphones_users_df[['user_id','gender','age']], on='user_id', how='left')
all_cellphones

"""# Data Preparation

Data preparation bertujuan untuk membersihkan dan mentransformasi data agar siap digunakan untuk pemodelan. Setelah itu, juga dilakukan penanganan nilai duplikat dan penyusunan ulang data agar sesuai dengan kebutuhan masing-masing model rekomendasi.
"""

all_cellphones.isnull().sum()

all_cellphones_clean = all_cellphones.dropna()
all_cellphones_clean

all_cellphones_clean.isnull().sum()

fix_cellphones = all_cellphones_clean.sort_values('cellphone_id', ascending=True)
fix_cellphones

len(fix_cellphones.cellphone_id.unique())

fix_cellphones.model.unique()

fix_cellphones[fix_cellphones['model'] == 'iPhone 13 Pro']

fix_cellphones[fix_cellphones['model'] == 'Galaxy S22']

preparation = fix_cellphones
preparation.sort_values('cellphone_id')

preparation = preparation.drop_duplicates('cellphone_id')
preparation

cellphone_id = preparation['cellphone_id'].tolist()
cellphone_model = preparation['model'].tolist()
cellphone_price = preparation['price'].tolist()
cellphone_brand = preparation['brand'].tolist()

print(len(cellphone_id))
print(len(cellphone_model))
print(len(cellphone_price))
print(len(cellphone_brand))

cellphone_new = pd.DataFrame({
    'id': cellphone_id,
    'model': cellphone_model,
    'price': cellphone_price,
    'brand': cellphone_brand
})
cellphone_new

"""# Model Development dengan Content Based Filtering

**Sistem rekomendasi dengan pendekatan Content-Based Filtering:** Model ini akan merekomendasikan produk berdasarkan kemiripan atribut atau konten dari produk itu sendiri.
"""

data = cellphone_new
data.sample(5)

"""## TF-IDF Vectorizer

**TF-IDF Vectorizer** berfungsi untuk mengubah data teks (dalam kasus ini, nama model cellphone) menjadi representasi numerik (vektor) yang dapat diukur kemiripannya.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(data['model'])

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(data['model'])

tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.brand
).sample(10, axis=0)

"""## Cosine Similarity

**Cosine Similarity** digunakan sebagai metrik untuk menghitung tingkat kemiripan antara vektor-vektor cellphone, sehingga kita bisa menemukan ponsel mana yang paling mirip satu sama lain.
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data.index, columns=data.index)
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def cellphone_recommendations(brand_name, similarity_data=cosine_sim_df, items=data[['brand', 'model']], k=5):
    brand_products = items[items['brand'] == brand_name]

    if brand_products.empty:
        print(f"Brand '{brand_name}' tidak ditemukan!")
        return pd.DataFrame()

    reference_idx = brand_products.index[0]

    sim_scores = list(enumerate(similarity_data.iloc[reference_idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    similar_indices = []
    for idx, score in sim_scores[1:]:
        if len(similar_indices) >= k:
            break
        similar_indices.append(idx)

    return items.iloc[similar_indices][['brand', 'model']]

data[data.brand.eq('Samsung')]

cellphone_recommendations('Samsung')

"""## Evaluation Precision@K"""

def precision_at_k(recommended_items, relevant_items, k):
  if k > len(recommended_items):
    raise ValueError("k cannot be greater than the number of recommended items.")

  recommended_at_k = recommended_items[:k]
  relevant_at_k = [item for item in recommended_at_k if item in relevant_items]

  return len(relevant_at_k) / k if k > 0 else 0

user_with_ratings = cellphones_ratings_df['user_id'].value_counts().idxmax()
user_ratings = cellphones_ratings_df[cellphones_ratings_df['user_id'] == user_with_ratings]

relevant_items_for_user = user_ratings[user_ratings['rating'] >= 4]['cellphone_id'].tolist()

if not relevant_items_for_user:
  print(f"User {user_with_ratings} has no relevant items for evaluation in this example.")
else:
  query_cellphone_id = relevant_items_for_user[0]
  query_cellphone_brand = cellphones_data_df[cellphones_data_df['cellphone_id'] == query_cellphone_id]['brand'].iloc[0]

  recommended_cellphones_df = cellphone_recommendations(query_cellphone_brand, k=10)

  if not recommended_cellphones_df.empty:
    recommended_item_ids = []
    for index, row in recommended_cellphones_df.iterrows():
        cellphone_id_match = cellphone_new[(cellphone_new['model'] == row['model']) & (cellphone_new['brand'] == row['brand'])]['id'].tolist()
        if cellphone_id_match:
            recommended_item_ids.append(cellphone_id_match[0])

    k_values = [1, 3, 5, 10]
    print(f"\nEvaluating Precision@K for recommendations based on brand '{query_cellphone_brand}' for user {user_with_ratings}:")
    for k in k_values:
      if len(recommended_item_ids) >= k:
        precision = precision_at_k(recommended_item_ids, relevant_items_for_user, k)
        print(f"Precision@{k}: {precision:.4f}")
      else:
        print(f"Cannot calculate Precision@{k} as fewer than {k} recommendations were generated.")
  else:
    print(f"No recommendations generated for brand '{query_cellphone_brand}'.")

"""# Model Development dengan Collaborative Filtering

Bagian ini akan fokus pada pembangunan sistem rekomendasi dengan pendekatan **Collaborative Filtering**, yang merekomendasikan item berdasarkan preferensi dari pengguna lain yang serupa.

## Data Understanding

Di sini, fokusnya adalah pada data rating dan pengguna, serta mengimpor library yang dibutuhkan untuk deep learning, seperti TensorFlow/Keras.
"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = cellphones_ratings_df
df

"""## Data Preparation

Langkah-langkah data preparation pada **Collaborative Filtering** meliputi encoding user_id dan cellphone_id menjadi indeks integer, normalisasi nilai rating, dan mengubah data menjadi format yang sesuai untuk dimasukkan ke dalam model deep learning.
"""

user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

cellphone_ids = df['cellphone_id'].unique().tolist()

cellphone_to_cellphone_encoded = {x: i for i, x in enumerate(cellphone_ids)}

cellphone_encoded_to_cellphone = {i: x for i, x in enumerate(cellphone_ids)}

df['user'] = df['user_id'].map(user_to_user_encoded)

df['cellphone'] = df['cellphone_id'].map(cellphone_to_cellphone_encoded)

num_users = len(user_to_user_encoded)
print(num_users)

num_cellphone = len(cellphone_encoded_to_cellphone)
print(num_cellphone)

df['rating'] = df['rating'].values.astype(np.float32)

min_rating = min(df['rating'])

max_rating = max(df['rating'])

print('Number of User: {}, Number of cellphone: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_cellphone, min_rating, max_rating
))

"""## Membagi Data untuk Training dan Validasi

**membagi dataset menjadi dua bagian: data latih (training set) dan data validasi (validation set).** Pembagian ini bertujuan untuk melatih model pada sebagian data dan menguji kinerjanya pada data yang belum pernah dilihat sebelumnya.
"""

df = df.sample(frac=1, random_state=42)
df

x = df[['user', 'cellphone']].values

y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],

    y[train_indices:]
)

print(x, y)

"""## Training

Pada proses ini dilakukanlah pembangunan arsitektur model deep learning **(Tensorflow Keras)**  dikompilasi dengan optimizer dan loss function, lalu dilatih menggunakan data latih yang telah disiapkan.
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_cellphone, embedding_size=64, dropout_rate=0.3, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_cellphone = num_cellphone
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-5)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.cellphone_embedding = layers.Embedding(
            num_cellphone,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-5)
        )
        self.cellphone_bias = layers.Embedding(num_cellphone, 1)

        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)

        self.dense1 = layers.Dense(128, activation='relu',
                                 kernel_regularizer=keras.regularizers.l2(1e-5))
        self.dense2 = layers.Dense(64, activation='relu',
                                 kernel_regularizer=keras.regularizers.l2(1e-5))
        self.dense3 = layers.Dense(32, activation='relu',
                                 kernel_regularizer=keras.regularizers.l2(1e-5))
        self.output_layer = layers.Dense(1, activation='sigmoid')

        self.bn1 = layers.BatchNormalization()
        self.bn2 = layers.BatchNormalization()

    def call(self, inputs, training=None):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        cellphone_vector = self.cellphone_embedding(inputs[:, 1])
        cellphone_bias = self.cellphone_bias(inputs[:, 1])

        user_vector = self.dropout1(user_vector, training=training)
        cellphone_vector = self.dropout2(cellphone_vector, training=training)

        dot_user_cellphone = tf.reduce_sum(user_vector * cellphone_vector, axis=1, keepdims=True)

        concat_features = layers.concatenate([
            user_vector,
            cellphone_vector,
            dot_user_cellphone,
            user_bias,
            cellphone_bias
        ])

        x = self.dense1(concat_features)
        x = self.bn1(x, training=training)
        x = self.dropout1(x, training=training)

        x = self.dense2(x)
        x = self.bn2(x, training=training)
        x = self.dropout2(x, training=training)

        x = self.dense3(x)
        output = self.output_layer(x)

        return output

model = RecommenderNet(num_users, num_cellphone, embedding_size=64, dropout_rate=0.3)

initial_learning_rate = 0.001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.9,
    staircase=True
)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    metrics=[
        tf.keras.metrics.RootMeanSquaredError(),
        tf.keras.metrics.BinaryAccuracy(),
        tf.keras.metrics.Precision(),
        tf.keras.metrics.Recall()
    ]
)

callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    # keras.callbacks.ReduceLROnPlateau(
    #     monitor='val_loss',
    #     factor=0.5,
    #     patience=8,
    #     min_lr=1e-7,
    #     verbose=1
    # ),
    keras.callbacks.ModelCheckpoint(
        'best_recommender_model.keras',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    )
]

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=callbacks,
    verbose=1
)

fig, axes = plt.subplots(1, 4, figsize=(20, 5))

axes[0].plot(history.history['root_mean_squared_error'])
axes[0].plot(history.history['val_root_mean_squared_error'])
axes[0].set_title('Model RMSE')
axes[0].set_ylabel('RMSE')
axes[0].set_xlabel('Epoch')
axes[0].legend(['train', 'validation'], loc='upper right')

axes[1].plot(history.history['binary_accuracy'])
axes[1].plot(history.history['val_binary_accuracy'])
axes[1].set_title('Model Binary Accuracy')
axes[1].set_ylabel('Accuracy')
axes[1].set_xlabel('Epoch')
axes[1].legend(['train', 'validation'], loc='upper right')

axes[2].plot(history.history['precision'])
axes[2].plot(history.history['val_precision'])
axes[2].set_title('Model Precision')
axes[2].set_ylabel('Precision')
axes[2].set_xlabel('Epoch')
axes[2].legend(['train', 'validation'], loc='upper right')

axes[3].plot(history.history['recall'])
axes[3].plot(history.history['val_recall'])
axes[3].set_title('Model Recall')
axes[3].set_ylabel('Recall')
axes[3].set_xlabel('Epoch')
axes[3].legend(['train', 'validation'], loc='upper right')

plt.tight_layout()
plt.show()

best_epoch_index = history.history['val_loss'].index(min(history.history['val_loss']))

final_rmse = history.history['root_mean_squared_error'][best_epoch_index]
final_binary_accuracy = history.history['binary_accuracy'][best_epoch_index]
final_precision = history.history['precision'][best_epoch_index]
final_recall = history.history['recall'][best_epoch_index]

print(best_epoch_index)
print(f"Nilai Akhir RMSE: {final_rmse:.4f}")
print(f"Nilai Akhir Binary Accuracy: {final_binary_accuracy:.4f}")
print(f"Nilai Akhir Precision: {final_precision:.4f}")
print(f"Nilai Akhir Recall: {final_recall:.4f}")

"""## Visualisasi Metrik

Hasil training model ditampilkan dalam bentuk visualisasi metrik performa yang diambil dari **Root Mean Squared Error (RMSE)** untuk menganalisis seberapa baik model belajar dan untuk mendeteksi adanya overfitting.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi Cellphone

Ini adalah tahap akhir dari proyek, yaitu penerapan model untuk menghasilkan rekomendasi. Kode di bawahnya akan mengambil ID pengguna secara acak, menggunakan model yang telah dilatih untuk memprediksi peringkat, dan menampilkan daftar 10 rekomendasi cellphone teratas.
"""

cellphone_df = cellphone_new
df = pd.read_csv('/kaggle/input/cellphones-recommendations/cellphones ratings.csv')

user_id = df.user_id.sample(1).iloc[0]
cellphone_visited_by_user = df[df.user_id == user_id]

cellphone_not_visited = cellphone_df[~cellphone_df['id'].isin(cellphone_visited_by_user.cellphone_id.values)]['id']
cellphone_not_visited = list(
    set(cellphone_not_visited)
    .intersection(set(cellphone_to_cellphone_encoded.keys()))
)

cellphone_not_visited = [[cellphone_to_cellphone_encoded.get(x)] for x in cellphone_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_cellphone_array = np.hstack(
    ([[user_encoder]] * len(cellphone_not_visited), cellphone_not_visited)
)

ratings = model.predict(user_cellphone_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_cellphone_ids = [
    cellphone_encoded_to_cellphone.get(cellphone_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('cellphone with high ratings from user')
print('----' * 8)

top_cellphone_user = (
    cellphone_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .cellphone_id.values
)

cellphone_df_rows = cellphone_df[cellphone_df['id'].isin(top_cellphone_user)]
for row in cellphone_df_rows.itertuples():
    print(row.brand, ':', row.model)

print('----' * 8)
print('Top 10 cellphone recommendation')
print('----' * 8)

recommended_cellphone = cellphone_df[cellphone_df['id'].isin(recommended_cellphone_ids)]
for row in recommended_cellphone.itertuples():
    print(row.brand, ':', row.model)